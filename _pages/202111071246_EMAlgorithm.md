---
title: EM Algorithm
permalink: /zettel/202111071246_EMAlgorithm
layout: page
tags: 

---
# EM Algorithm

The EM Algorithm is used to simplify difficult [maximum likeylhood problems](202111071235_MaximumLikelyhoodEstimation).

By introducing latent (hidden) variables, the maximisation of the [log-likelyhood](202101091603_probabilityLikelyhood) can become tracteable.

The steps can be seen [using a KL-Divergence term](202111251825_EMAlgorithmStepsusingKLDivergence).

**Steps:**

In the **E-Step** we work out the posterior of our latent variables with respect to the current parameter values. Ie
$$
p(z \vert x, \theta_{current})
$$

We then work out an expectation of the full joint log-likelyhood with respect to this posterior distribution. Ie
$$
E_{q(z)} \big[ \log p(X, Z) ] = \sum_i E_{q(z)} \big[ \log p(x_i, z \vert \theta) \big]
$$
Where $q(z)$ is the posterior defined above. Further simplifications using conditional probability can usually be made. 

In the **M-Step** we then maximise this expectation with respect to parameters to get $\theta_{new}$.

We then calculate the current log-likelyhood and test for convergence. If we have not converged yet, we proceed back to the E-Step. The full log-likelyhood being:
$$
\log p(X \vert \theta) = \sum_i \log p(x_i \vert \theta) = \sum_i \log \sum_z p(x_i, z \vert \theta)
$$
Again this term is usually expanded further to put in terms of the model.

Links: 

References: 

