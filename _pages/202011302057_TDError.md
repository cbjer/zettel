---
title: Zettels
permalink: /zettel/202011302057_TDError
layout: default
---
tags: #update #gradient #onestep

# TD Error

The TD Error is defined as 

$$
\delta_t := R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
$$

We can think of it as the difference between our current estimate for the value 
of $S_t$, $V(S_t)$ and the better estimate using one step of actual reward 
$R_{t+1} + \gamma V(S_{t+1})$.

There is also an state-action value version of TD Error:

$$ \delta_t := R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) $$

Links: [Tabular TD(0)](202011302050_tabularTDZero)

References: Introduction to Reinforcement Learning - Sutton and Barto

[Return to Index](index)