---
title: Cross Entropy
permalink: /zettel/202103271307_crossEntropy
layout: page
tags: loss

---
# Cross Entropy

The **cross entropy** is the expected number of bits to communicate the values taken by a 
random variable $X$ with probability mass function $g(x)$ but where we are using a sub-optimal 
coding scheme $f(x)$.

$$
H(g, f) = - \sum_x g(x) \log f(x)
$$

Also thought of as our model thinks the data is distributed as $f(x)$ but is really distributed according to 
$g(x)$.

Links: [Entropy](202103271322_entropy)

References: [Connections: Log Likelihood, Cross Entropy, KL Divergence, Logistic Regression, and Neural Networks](https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/)

