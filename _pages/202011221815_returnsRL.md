---
title: Returns in Reinforcement Learning
permalink: /zettel/202011221815_returnsRL
layout: page
---
tags: #reinforcementlearning \

# Returns in Reinforcement Learning

Given we are in a state $S_t$ we are interested in the sequence of future [rewards](TODOs) $ \{ R_{t+1}, R_{t+2}, \dots \} $.

In Reinforcement Learning, we want to **maximise the expected return** where return $G_t$ is some function of the reward sequence.

In a simple case where we have a finite [episode](TODOs) we could define return as:

$$ G_t := R_{t+1} + R_{t+2} + \dots + R_{T} $$

where $T$ is our terminal state.

If however we have episodes of infinite length, we more commonly use the **discounted return** formulation:

$$ 
\begin{aligned} 
G_t &:= R_{t+1} + \gamma R_{t+2} + \dots \\ 
&= \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{aligned}
$$

where $\gamma$ is our discount rate. $ 0 \leq \gamma \leq 1$. Most commonly if we refer to the "return", we are referring to this discounted return formulation.

Links: []

References: Introduction to Reinforcement Learning - Sutton and Barto

[Return to Index](index)