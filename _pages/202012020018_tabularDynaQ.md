---
title: Tabula Dyna-Q
permalink: /zettel/202012020018_tabularDynaQ
layout: page
tags: planning model

---
# Tabula Dyna-Q

**Dyna-Q** is a form of [model based learning](202012012352_modelBasedRL) where 
we introduce a step of sampling simulated rewarda and next states from our model for 
a given randomly selected action and state. In this we assume the environment is deterministic and therefore
we simply store down previously experienced rewards and following states $s'$ for some given state-action pair.

We call these updates which use simulated experience [planning updates](202012012357_rlPlanning).

Here the **learning** and **planning** stages are completed by exactly the same algorithm,
specifically [Q-Learning](202011302128_qLearning).

<figure>
  <img src="/zettel/Images/ReinforcementLearning/TabularDynaQ.png"
     alt="ALT"
     class="centerImage"
     style="width: 700px;" />
  <figcaption> Tabular Dyna-Q </figcaption>     
</figure>

Note that in this algorithm, we assume the environment is deterministic. Ie we know exactly the dynamics of choosing action $a$ in 
state $s$ lead to reward $r$ and next state $s'$. This could be changed with making 
the model a distribution, and assume we are sampling from it. 

An extension to the algorithm is adding a reward for trying state-action pairs which 
we havn't tried on the real environment in some time, eg $\sqrt{\tau(S_t, a)}$, with $\tau$ 
corresponding to the number of time steps since that action-pair has been used.

Links: []

References: Introduction to Reinforcement Learning - Sutton and Barto

[Return to Index](index)