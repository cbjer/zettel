---
title: Bumping
permalink: /zettel/202111252245_Bumping
layout: page
tags: learning

---
# Bumping

Similiar to [bagging](202111252019_Bagging), we draw [bootstrapped](202101161648_theBootstrap) samples from the training set and fit a model to each sample. 

But we then choose the model which best fits the training set.

Eg in a regression setting:
$$
\hat{b} = \argmin_b \sum_{i=1}^N [y_i - \hat{f}^b(x_i)]^2
$$

then choosing $\hat{f}^{\hat{b}}(x)$ as our final model.

Usually the full training set is also added as one of the bootstrapped samples so this can be picked if best.

(I think) this would be applied on learning methods which don't have a guarantee of a global minimum and instead can get stuck in poor local optimals. 

Links: 

References: 

