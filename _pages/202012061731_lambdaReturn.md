---
title: Lambda-Return
permalink: /zettel/202012061731_lambdaReturn
layout: page
tags: return target

---
# Lambda-Return

The $\lambda$-return is a [return](202011221815_returnsRL) formulation 
which forms a weighted sum of [n-step returns](202011302230_nstepReturn). 
The $\lambda$ parameter controls how much weighting we put on shorter 
bootstrapped returns and the longer timestep. At the extremes we have $\lambda=0$ 
corresponding to [TD(0)](202011302050_tabularTDZero) and $\lambda=1$ corresponding 
to a full [return](202011221815_returnsRL) as used in [monte-carlo methods](202011301233_monteCarloExploringStarts).

$$
G_t^{\lambda} := (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+n}
$$

Using the parameterised definition for the n-step return:
$$
G_{t:t+n} := \sum_{i=1}^{n} \gamma^{i-1} R_{t+i} + \gamma^n \hat{v}(S_{t+n})
$$

we can show a 1-step relation of:
$$
G_t^{\lambda} = R_{t+1} + \gamma ( 1- \lambda ) \hat{v}(S_{t+1}) + \gamma \lambda G_{t+1}^{\lambda}
$$

Links: []

References: Introduction to Reinforcement Learning - Sutton and Barto

