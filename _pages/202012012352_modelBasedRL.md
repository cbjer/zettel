---
title: Model-Based Reinforcment Learning
permalink: /zettel/202012012352_modelBasedRL
layout: page
---
tags: #planning #sample

# Model-Based Reinforcment Learning

Model based Reinforcement Learning involves a separate model which is trying to emulate / simulate the dynamics 
of the full environment. It can be seen as some prior knowledge we have of the environment which we 
give to our agent. The model can also be updated as we encounter real experience from the environment. 

For example, in Chess, we know what the resulting state will be after choosing an action. We do not 
need our agent to understand that the action "Knight to A3" results in a state with the 
knight at A3. 

Model based RL methods also typically involve [planning](202012012357_rlPlanning). They 
are also sometimes referred to "indirect reinforcement learning" as opposed to 
[model-free RL](202012020011_modelFreeRL).

We also have a distinction between **distribution models** and **sample models**. With distribution models our model describes 
all possibilites and their probabilities, ie directly modeling $p(s', r | s, a)$, the process governing 
the environment dynamics. Sample models instead produce just 1 of the possibilies. Distribution models are 
stronger but much harder to obtain.

Model based learning has a form of [exploration vs exploitation dilemma](TODOs) in
that there is a tradeoff between behaving best with respects to current model (exploitation)
and improving the model (exploration).

Links: [Model-Free RL](202012020011_modelFreeRL)

References: Introduction to Reinforcement Learning - Sutton and Barto

[Return to Index](index)