---
title: Greedy Policy to enable Policy Improvement
permalink: /zettel/202011292245_greedyPolicy
layout: page
---
tags: #maximisation #onestep

# Greedy Policy to enable Policy Improvement

We define the deterministic greedy policy as

$$
\pi'(s) := \arg \max_{a} q_{\pi}(s,a)
$$

which takes the action corresponding to the highest state-action value for a given state. 

Writing the value function of policy $\pi$ in terms of 
[the state action value function](202011221924_expressingStateToActionValue), we can write

$$
\begin{aligned}
v_{\pi}(s) &= \sum_{a'} \pi(a' | s) q_{\pi}(s,a') \\
& \leq \sum_{a'} \pi(a'|s) \max_{a} q_{\pi} (s, a) \\
&= \max_{a} q_{\pi}(s,a) \\
&= q_{\pi}(s, \pi'(s))
\end{aligned}
$$

Using the [policy improvement theorem](202011292146_policyImprovement), this shows 
the greedy policy is always [equal or better](202011252200_partialOrderingPolicies) 
to the existing policy.

We can see it another way through:

$$ q_{\pi}(s, \arg\max_a q_{\pi}(s,a)) = \max_a q_{\pi}(s,a) \geq q_{\pi}(s, \pi(s)) = v_{\pi}(s) $$

If the new greedy policy is equal but not better than the existing policy, then 
it follows that this policy is a solution to the [bellman optimality equation](202011262156_bellmanOptimalityStateValue) 
and therefore is an optimal policy.

Links: []

References: Introduction to Reinforcement Learning - Sutton and Barto

[Return to Index](index)