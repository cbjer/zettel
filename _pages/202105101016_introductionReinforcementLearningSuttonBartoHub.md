---
title: Introduction to Reinforcement Learning - Sutton & Barto - Hub
permalink: /zettel/202105101016_introductionReinforcementLearningSuttonBartoHub
layout: page
tags: reinfocement learning

---
# Introduction to Reinforcement Learning - Sutton & Barto - Hub

[Reinforcement Learning Returns](202011221815_returnsRL)

[Value Function](202011221845_valueFunctions)

[Action-Value function](202011221903_actionValueFunction)

[Expressing State-value function in terms of the state-action value function](202011221924_expressingStateToActionValue)

[Recursive relation for returns](202011222109_recursiveReturns)

[MDP Short-hand notation](202011222137_mdpShortNotation)

[Bellman Equation for State-values](202011222150_bellmanEquationStateValues)

[Reinforcement Learning Policy](202011242107_rlPolicy)

[Markov Property States](202011242210_markovStates)

[Markov Process](202011242218_markovProcess)

[Bellman Equation for State-Action values](202011242151_bellmanStateAction)

[Partial ordering of policies](202011252200_partialOrderingPolicies)

[Optimal Value Functions](202011262033_optimalValueFunctions)

[Expressing state-action value function in terms of state value function](202011262125_stateActionTostatevalue)

[Bellman Optimality Equation for State Values](202011262156_bellmanOptimalityStateValue)

[Bellman Optimality Equation for State-Action Values](202011291755_bellmanOptimalityStateActionValue)

[Iterative Policy Evaluation for finding value function](202011291938_iterativePolicyEvaluation)

[Policy Improvement](202011292146_policyImprovement)

[Greedy Policy to Enable Policy Improvement](202011292245_greedyPolicy)

[Policy Iteration](202011292319_policyIteration)

[Value Iteration](202011292335_valueIteration)

[First Visit Monte Carlo Prediction](202011301143_firstVisitMonteCarloPrediction)

[Monte Carlo with Exploring Starts](202011301233_monteCarloExploringStarts)

[Epsilon-Greedy Policy](202011301251_epsilonGreedyPolicy)

[On-Policy Methods](202011301310_onPolicyMethods)

[Off-Policy Methods](202011301312_offPolicyMethods)

[On-Policy First Visit Monte Carlo Control](202011301316_onPolicyFirstVisitMCControl)

[Off-Policy Monte Carlo Prediction](202011301626_offPolicyMCPrediction)

[Off-Policy Monte Carlo Control](202011301638_offPolicyMCControl)

[Tabular TD(0)](202011302050_tabularTDZero)

[TD Error](202011302057_TDError)

[Sarsa](202011302117_sarsa)

[Q-Learning](202011302128_qLearning)

[Expected Sarsa](202011302147_expectedSarsa)

[Double Q-Learning](202011302204_doubleQLearning)

[Maximasation Bias](202011302212_maximisationBias)

[n-step TD](202011302226_nStepTD)

[n-step Return](202011302230_nstepReturn)

[n-step Sarsa](202011302235_nStepSarsa)

[Off-Policy n-step Sarsa](202011302245_offPolicynstepSarsa)

[n-step Tree Backup](202012012205_nstepTreeBackup)

[Off-policy n-step Q(sigma)](202012012214_offPolicynstepQSigma)

[Model Based Reinforcement Learning](202012012352_modelBasedRL)

[Planning](202012012357_rlPlanning)

[Model-free Reinforcment Learning](202012020011_modelFreeRL)

[Tabular Dyna-Q](202012020018_tabularDynaQ)

[Prioritized Sweeping](202012022123_prioritizedSweeping)

[Trajectory Sampling](202012022327_trajectorySampling)

[Decision-time Planning](202012022346_decisionTimePlanning)

[Rollout Algorithm](202012031744_rolloutAlgorithm)

[Monte Carlo Tree Search](202012032005_monteCarloTreeSearch)

[Mean Squared Value Error](202012032202_meanSquaredValueError)

[SGD on Value Function Approximators](202012032217_sgdValueFunction)

[Gradient Monte Carlo](202012032231_gradientMonteCarlo)

[Semi-gradient TD(0)](202012032232_semigradientTDZero)

[Linear Value Function Approximator](202012032318_linearValueFunctionApproximator)

[n-Step Semi Gradient TD](202012032327_nstepSemiGradientTD)

[Least-Squares TD](202012040022_leastSquaresTD)

[Reinforcement Learning as Supervised Learning](202012041144_rlAsSupervisedLearning)

[Encoding the State Space as Feature Vectors](202012041222_featureVectorStates)

[Episodic Semi-Gradient Sarsa](202012052205_episodicSemiGradientSarsa)

[Function Approximators](202012052211_rlFunctionApproximators)

[Episodic Semi-Gradient n-step Sarsa](202012052243_episodicSemiGradientnStepSarsa)

[Differential Semi-Gradient Sarsa](202012052308_differentialSemiGradientSarsa)

[Differential Semi-Gradient n-step Sarsa](202012052310_differentialSemiGradientnStepSarsa)

[Off-Policy with Function Approximators](202012061310_offPolicyFunctionApproximators)

[Lambda Return](202012061731_lambdaReturn)

[TD Lambda](202012061731_tdLambda)

[True Online TD Lambda](202012061731_trueLambdaTDLambda)

[Sarsa Lambda](202012061732_sarsaLambda)

[True Online Sarsa Lambda](202012061732_trueOnlineSarsaLambda)

[Eligibility Traces](202012061733_eligibilityTraces)

[Forward View vs Backward View](202012061733_forwardViewVsBackwardView)

[REINFORCE Algorithm](202012121511_reinforceAlgorithm)

[REINFORCE with Baseline](202012121514_reinforceWithBaseline)

[One-step Actor Critic](202012121514_oneStepActorCritic)

[Actor Critic with Eligibility Traces - Episodic](202012121515_actorCriticEligibilityTracesEpisodic)

[Actor Critic with Eligibility Traces - Continuing](202012121516_actorCriticEligibilityTracesContinuing)

[Policy Gradient Methods](202012141139_policyGradientMethods)

[Softmax in Action Preferences](202012141156_softmaxActionPreferences)

[Policy Gradient Theorem](202012141215_policyGradientTheorem)

[Actor-Critic Methods](202012141327_actorCriticMethods)

[Derivation of Policy Gradient Method Updates](202012141337_derivationPolicyGradientUpdates)

[Examples of RL Applications](202012142052_examplesRLApplications)


Links: 

References: 

[Return to Index](index)