---
title: Entropy
permalink: /zettel/202103271322_entropy
layout: page
tags: definition

---
# Entropy

The **entropy** $H(x)$ of a random variable $X$ with probability mass function $f(x)$ is defined as:

$$
H(X) = \sum_x f(x) \log \frac{1}{f(x)} = - \sum_x f(x) \log f(x)
$$

It is the expected number of bits necessary to communicate the value taken by $X$ if we 
use the best possible coding scheme for the distribution $f(x)$.


Often called the **Shannon Entropy**.

Links: [Cross Entropy](202103271307_crossEntropy)

References: [Connections: Log Likelihood, Cross Entropy, KL Divergence, Logistic Regression, and Neural Networks](https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/)

[Return to Index](index)