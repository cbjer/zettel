---
title: Bagging
permalink: /zettel/202111252019_Bagging
layout: page
tags: sampling

---
# Bagging

Bagging or **[Bootstrap](202101161648_theBootstrap) aggregation**, is a technique whereby we train multiple learners using different bootstrapped samples.

Ie train multiple weak learners via sampling with replacement from the dataset.

In the regression setting, this would usually be done by averaging the output across the bootstrapped learners. This reduces the variance of the output. 

Heavily linked to Tree based methods.

In the classification setting, one method is to give each of the learners a single vote for which category $x$ belongs to. Then the overall learner selects the category with the most votes.

With some learners you may get class proportions instead, so can instead output a distribution over categories.

Bagging can dramatically **reduce the variance** of unstable procedures such as trees. 

Links: 

References: 

