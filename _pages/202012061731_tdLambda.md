---
title: TD($\lambda$)
permalink: /zettel/202012061731_tdLambda
layout: page
---
tags: #algorithm #bootstrapping

# TD($\lambda$)

The TD($\lambda$) algorithm generalises a [lambda-return](202012061731_lambdaReturn) 
update to the original [temporal-difference](202011302050_tabularTDZero) 
based learning algorithm. Notably, the TD($\lambda$) algorithm uses a [backward view](202012061733_forwardViewVsBackwardView)
so that we can compute updates online, rather than waiting for the end of the episode. However,
TD($\lambda$) only approximates the forward view, [true online TD(Lambda)](202012061731_trueLambdaTDLambda) 
has the exact equivalence to the forward view.

<figure>
  <img src="/zettel/Images/ReinforcementLearning/SemiGradientTDLambdaV.png"
     alt="ALT"
     class="centerImage"
     style="width: 700px;" />
  <figcaption> Semi-Gradient TD Lambda </figcaption>     
</figure>


Links: []

References: Introduction to Reinforcement Learning - Sutton and Barto

[Return to Index](index)