---
title: KL Divergence
permalink: /zettel/202111251549_KLDivergence
layout: page
tags: definition information

---
# KL Divergence

The **Kullback-Leibler Divergence** is a measure for how different two probability distributions are. Note it is non-symmetric.

$$
D_{KL} \big( p(x) \vert \vert q(x) \big) = E_{p(x)} \Big[ \log \frac{p(x)}{q(x)} \Big] = \sum_x p(x) \log \frac{p(x)}{q(x)}
$$

Intuitively can be thought of as the information lost when $q(x)$ is used to approximate $p(x)$.

Linked to [Cross-Entropy](202103271307_crossEntropy)


Links: 

References: 

