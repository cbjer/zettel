---
title: Zettels
permalink: /zettel/202011302230_nstepReturn
layout: default
---
tags: #return #nstep

# n-step Return

The definition of n-step return is 

$$ G_{t : t+n} := R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n} + \gamma^nV_{t+n-1}(S_{t+n}) $$

We can think of the case $n=1$ corresponding to [TD(0)](202011302050_tabularTDZero).

We also have an alternative form used for state-action values. This is used in [n-step Sarsa](202011302235_nStepSarsa).

$$ G_{t : t+n} := R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n} + \gamma^nQ_{t+n-1}(S_{t+n}, A_{t+n}) $$

A further formulation, used for [n-step expected Sarsa](TODO), is

$$
\begin{aligned}
G_{t : t+n} &:= R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n \bar{V}_{t+n-1}(S_{t+n}) \\
\bar{V}_t(s) &:= \sum_a \pi(a | s) Q_t(s, a) 
\end{aligned}
$$

Where we have used an expected value of the state, as is described by our current state-action value function.

Links: []

References: Introduction to Reinforcement Learning - Sutton and Barto

[Return to Index](index)